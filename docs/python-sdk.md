# ğŸš€ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®ØªØ§Ù…ÙŠØ© - Python SDK & Development
ğŸ“„ Ø§Ù„Ù…Ù„Ù 32: `docs/python-sdk.md`

---

# ğŸ Bisheng Enterprise Python SDK

## Ø§Ù„ØªØ«Ø¨ÙŠØª

```bash
pip install bisheng-enterprise-sdk
```

Ø£Ùˆ Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±:

```bash
git clone https://github.com/yourusername/bisheng-python-sdk.git
cd bisheng-python-sdk
pip install -e .
```

---

ğŸš€ Ø§Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø³Ø±ÙŠØ¹

Python

from bisheng import BishengClient

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù…ÙŠÙ„
client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    api_key="your-api-key"
)

# Ø±ÙØ¹ ÙˆØ«ÙŠÙ‚Ø©
document = client.documents.upload(
    file_path="research-paper.pdf",
    collection="research",
    extract_tables=True,
    ocr=True,
    language="ara"
)

print(f"Document uploaded: {document.id}")

# Ø§Ù„Ø¨Ø­Ø«
results = client.search.text(
    query="Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    collection="research",
    limit=10
)

for result in results:
    print(f"- {result.filename} (score: {result.score})")

# Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨
answer = client.chat.ask(
    question="Ù…Ø§ Ù‡ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ",
    collection="research",
    model="gpt-4"
)

print(f"Answer: {answer.text}")

ğŸ“š Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
1. Ø§Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø©
Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

Python

from bisheng import BishengClient

# Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… API Key
client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    api_key="your-api-key"
)

# Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙˆÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±
client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    email="admin@bisheng.io",
    password="your-password"
)

# Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    api_key="your-api-key",
    timeout=60,  # Ø«ÙˆØ§Ù†ÙŠ
    verify_ssl=True,
    max_retries=3
)

ØªØ¬Ø¯ÙŠØ¯ Token ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹

Python

client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    email="admin@bisheng.io",
    password="your-password",
    auto_refresh_token=True  # ØªØ¬Ø¯ÙŠØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ
)

2. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
Ø±ÙØ¹ ÙˆØ«ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©

Python

# Ø±ÙØ¹ Ø¨Ø³ÙŠØ·
document = client.documents.upload("paper.pdf")

# Ø±ÙØ¹ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
document = client.documents.upload(
    file_path="research-paper.pdf",
    collection="research-2024",
    metadata={
        "author": "Dr. Ahmed",
        "year": 2024,
        "department": "AI Research",
        "tags": ["machine-learning", "nlp", "arabic"]
    },
    extract_tables=True,
    extract_images=True,
    ocr=True,
    language="ara",
    chunk_size=1000,
    chunk_overlap=200
)

print(f"Document ID: {document.id}")
print(f"Status: {document.status}")
print(f"Pages: {document.pages}")

Ø±ÙØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚

Python

import os
from pathlib import Path

# Ø±ÙØ¹ ÙƒÙ„ Ù…Ù„ÙØ§Øª PDF ÙÙŠ Ù…Ø¬Ù„Ø¯
pdf_files = Path("./documents").glob("*.pdf")

uploaded = []
for pdf_file in pdf_files:
    try:
        doc = client.documents.upload(
            file_path=str(pdf_file),
            collection="research-2024"
        )
        uploaded.append(doc)
        print(f"âœ“ Uploaded: {pdf_file.name}")
    except Exception as e:
        print(f"âœ— Failed: {pdf_file.name} - {e}")

print(f"\nTotal uploaded: {len(uploaded)}")

Ø±ÙØ¹ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ (Parallel Upload)

Python

from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

def upload_file(file_path):
    """Ø±ÙØ¹ Ù…Ù„Ù ÙˆØ§Ø­Ø¯"""
    try:
        doc = client.documents.upload(
            file_path=str(file_path),
            collection="research-2024"
        )
        return {"success": True, "file": file_path.name, "doc": doc}
    except Exception as e:
        return {"success": False, "file": file_path.name, "error": str(e)}

# Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
pdf_files = list(Path("./documents").glob("*.pdf"))

# Ø±ÙØ¹ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ (5 Ù…Ù„ÙØ§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª)
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(upload_file, f) for f in pdf_files]
    
    for future in as_completed(futures):
        result = future.result()
        if result["success"]:
            print(f"âœ“ {result['file']} - ID: {result['doc'].id}")
        else:
            print(f"âœ— {result['file']} - Error: {result['error']}")

Ù…Ø±Ø§Ù‚Ø¨Ø© ØªÙ‚Ø¯Ù… Ø§Ù„Ø±ÙØ¹

Python

import time

# Ø±ÙØ¹ ÙˆØ«ÙŠÙ‚Ø©
document = client.documents.upload(
    file_path="large-document.pdf",
    collection="research"
)

# Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
print("Processing document...")
while document.status in ["processing", "queued"]:
    document.refresh()  # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
    print(f"Progress: {document.progress}% - Status: {document.status}")
    time.sleep(2)

if document.status == "completed":
    print(f"âœ“ Document processed successfully!")
    print(f"  - Pages: {document.pages}")
    print(f"  - Tables: {document.tables_count}")
    print(f"  - Images: {document.images_count}")
    print(f"  - Processing time: {document.processing_time_seconds}s")
else:
    print(f"âœ— Processing failed: {document.error_message}")

Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ«ÙŠÙ‚Ø©

Python

# Ø¨ÙˆØ§Ø³Ø·Ø© ID
document = client.documents.get("doc_abc123")

print(f"Filename: {document.filename}")
print(f"Status: {document.status}")
print(f"Pages: {document.pages}")
print(f"Size: {document.size_bytes / 1024 / 1024:.2f} MB")
print(f"Created: {document.created_at}")
print(f"Metadata: {document.metadata}")

Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚

Python

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
documents = client.documents.list()

for doc in documents:
    print(f"- {doc.filename} ({doc.status})")

# Ù…Ø¹ ÙÙ„ØªØ±Ø©
documents = client.documents.list(
    collection="research-2024",
    status="completed",
    page=1,
    page_size=50,
    sort_by="created_at",
    order="desc"
)

print(f"Total: {documents.total}")
print(f"Page: {documents.page}/{documents.total_pages}")

# ØªØµÙØ­ ÙƒÙ„ Ø§Ù„ØµÙØ­Ø§Øª
all_docs = []
for page in range(1, documents.total_pages + 1):
    page_docs = client.documents.list(page=page, page_size=100)
    all_docs.extend(page_docs.items)

print(f"Total documents: {len(all_docs)}")

ØªØ­Ø¯ÙŠØ« Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ«ÙŠÙ‚Ø©

Python

# ØªØ­Ø¯ÙŠØ« metadata
document = client.documents.update(
    document_id="doc_abc123",
    metadata={
        "reviewed": True,
        "reviewer": "Dr. Sarah",
        "review_date": "2024-01-15"
    }
)

Ø­Ø°Ù ÙˆØ«ÙŠÙ‚Ø©

Python

# Ø­Ø°Ù ÙˆØ«ÙŠÙ‚Ø© ÙˆØ§Ø­Ø¯Ø©
client.documents.delete("doc_abc123")

# Ø­Ø°Ù Ù…ØªØ¹Ø¯Ø¯
doc_ids = ["doc_abc123", "doc_def456", "doc_ghi789"]
for doc_id in doc_ids:
    try:
        client.documents.delete(doc_id)
        print(f"âœ“ Deleted: {doc_id}")
    except Exception as e:
        print(f"âœ— Failed to delete {doc_id}: {e}")

3. Ø§Ù„Ø¨Ø­Ø«
Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ (Full-text Search)

Python

# Ø¨Ø­Ø« Ø¨Ø³ÙŠØ·
results = client.search.text(
    query="Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
    collection="research"
)

for result in results:
    print(f"Score: {result.score:.2f}")
    print(f"File: {result.filename}")
    print(f"Page: {result.page}")
    print(f"Snippet: {result.snippet}")
    print("-" * 50)

# Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ÙÙ„ØªØ±Ø©
results = client.search.text(
    query="machine learning",
    collection="research-2024",
    filters={
        "year": {"gte": 2020, "lte": 2024},
        "author": {"in": ["Dr. Ahmed", "Dr. Sarah"]},
        "tags": {"contains": "nlp"}
    },
    limit=20,
    offset=0,
    highlight=True,
    include_metadata=True
)

# Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
print(f"Total results: {results.total}")
for i, result in enumerate(results, 1):
    print(f"\n{i}. {result.filename}")
    print(f"   Score: {result.score:.3f}")
    print(f"   Author: {result.metadata.get('author')}")
    print(f"   Snippet: {result.snippet[:100]}...")

Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Semantic Search)

Python

# Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ
results = client.search.semantic(
    query="applications of artificial intelligence in modern education systems",
    collection="research",
    similarity_threshold=0.7,
    limit=10,
    rerank=True
)

for result in results:
    print(f"Similarity: {result.similarity:.3f}")
    print(f"File: {result.filename}")
    print(f"Text: {result.text[:200]}...")
    print()

# Ø¨Ø­Ø« Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª
results_ar = client.search.semantic(
    query="ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    collection="research",
    language="ara"
)

Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®ØªÙ„Ø· (Hybrid Search)

Python

# Ø¯Ù…Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ ÙˆØ§Ù„Ø¯Ù„Ø§Ù„ÙŠ
results = client.search.hybrid(
    query="neural networks deep learning",
    collection="research",
    text_weight=0.3,      # ÙˆØ²Ù† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ
    semantic_weight=0.7,  # ÙˆØ²Ù† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    limit=10
)

for result in results:
    print(f"Combined Score: {result.combined_score:.3f}")
    print(f"  - Text Score: {result.text_score:.3f}")
    print(f"  - Semantic Score: {result.semantic_score:.3f}")
    print(f"File: {result.filename}")
    print()

Ø¨Ø­Ø« Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ pagination

Python

# Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
def search_all(query, collection, page_size=50):
    """Ø¬Ù„Ø¨ ÙƒÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«"""
    all_results = []
    offset = 0
    
    while True:
        results = client.search.text(
            query=query,
            collection=collection,
            limit=page_size,
            offset=offset
        )
        
        if not results.items:
            break
        
        all_results.extend(results.items)
        offset += page_size
        
        print(f"Fetched {len(all_results)}/{results.total} results...")
        
        if len(all_results) >= results.total:
            break
    
    return all_results

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
all_results = search_all("AI", "research")
print(f"Total results found: {len(all_results)}")

4. Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨ Ø¨Ø³ÙŠØ·

Python

# Ø³Ø¤Ø§Ù„ Ø¨Ø³ÙŠØ·
answer = client.chat.ask(
    question="Ù…Ø§ Ù‡ÙŠ ÙÙˆØ§Ø¦Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ØŸ",
    collection="research"
)

print(f"Q: {answer.question}")
print(f"A: {answer.text}")
print(f"\nSources:")
for source in answer.sources:
    print(f"  - {source.filename} (page {source.page})")

Ø³Ø¤Ø§Ù„ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©

Python

answer = client.chat.ask(
    question="What are the main challenges in implementing AI in education?",
    collection="research-2024",
    model="gpt-4",
    temperature=0.7,
    max_tokens=500,
    system_prompt="You are an expert in educational technology. Provide detailed, academic answers.",
    include_sources=True,
    language="en"
)

print(f"Answer: {answer.text}")
print(f"Model: {answer.model}")
print(f"Tokens used: {answer.tokens_used}")
print(f"Response time: {answer.took_ms}ms")

Streaming Response

Python

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠ
stream = client.chat.ask(
    question="Ø´Ø±Ø­ Ù…ÙØµÙ„ Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©",
    collection="research",
    stream=True
)

print("Answer: ", end="", flush=True)
for chunk in stream:
    print(chunk.content, end="", flush=True)
print("\n")

Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Ø±

Python

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©
conversation = client.chat.create_conversation(
    collection="research",
    model="gpt-4"
)

# Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ø¦Ù„
response1 = conversation.send("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ")
print(f"Bot: {response1.text}\n")

response2 = conversation.send("Ù…Ø§ Ù‡ÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ØŸ")
print(f"Bot: {response2.text}\n")

response3 = conversation.send("Ø£Ø¹Ø·Ù†ÙŠ Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©")
print(f"Bot: {response3.text}\n")

# Ø¹Ø±Ø¶ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
print("\nConversation History:")
for msg in conversation.messages:
    print(f"{msg.role}: {msg.content}")

# Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
conversation.save()

# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©
old_conversation = client.chat.get_conversation("conv_xyz789")

5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ

Python

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ
text = client.documents.extract_text("doc_abc123")
print(f"Total words: {text.total_words}")
print(f"Content: {text.full_text[:500]}...")

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙØ­Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
text = client.documents.extract_text(
    document_id="doc_abc123",
    pages="1,2,3"  # Ø£Ùˆ "1-5"
)

for page in text.pages:
    print(f"\n--- Page {page.page_number} ---")
    print(page.text)
    print(f"Word count: {page.word_count}")

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹ ØªÙ†Ø³ÙŠÙ‚
markdown_text = client.documents.extract_text(
    document_id="doc_abc123",
    format="markdown"
)

Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„

Python

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
tables = client.documents.extract_tables("doc_abc123")

print(f"Found {len(tables)} tables")

for i, table in enumerate(tables, 1):
    print(f"\nTable {i} (Page {table.page}):")
    print(f"Size: {table.rows}x{table.columns}")
    
    # Ø¹Ø±Ø¶ ÙƒÙ€ DataFrame
    import pandas as pd
    df = pd.DataFrame(table.data[1:], columns=table.data[0])
    print(df)
    
    # Ø­ÙØ¸ ÙƒÙ€ CSV
    df.to_csv(f"table_{i}.csv", index=False)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯ÙˆÙ„ Ù…Ø¹ÙŠÙ†
table = client.documents.extract_tables(
    document_id="doc_abc123",
    page=5,
    table_index=0
)

Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±

Python

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„ØµÙˆØ±
images = client.documents.extract_images("doc_abc123")

print(f"Found {len(images)} images")

for i, image in enumerate(images, 1):
    print(f"Image {i}:")
    print(f"  Page: {image.page}")
    print(f"  Size: {image.width}x{image.height}")
    print(f"  Format: {image.format}")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
    image_data = image.download()
    with open(f"image_{i}.{image.format}", "wb") as f:
        f.write(image_data)
    
    # Ø£Ùˆ Ø­ÙØ¸ Ù…Ø¨Ø§Ø´Ø±Ø©
    image.save(f"image_{i}.{image.format}")

ØªÙ„Ø®ÙŠØµ

Python

# ØªÙ„Ø®ÙŠØµ ÙˆØ«ÙŠÙ‚Ø©
summary = client.documents.summarize(
    document_id="doc_abc123",
    max_length=500,
    language="ar",
    style="academic"
)

print(f"Original: {summary.original_word_count} words")
print(f"Summary: {summary.word_count} words")
print(f"Compression: {summary.compression_ratio:.1%}")
print(f"\n{summary.text}")

# ØªÙ„Ø®ÙŠØµ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª
summaries = client.documents.summarize(
    document_id="doc_abc123",
    levels=["brief", "detailed", "comprehensive"]
)

for level, summary in summaries.items():
    print(f"\n{level.upper()} Summary ({summary.word_count} words):")
    print(summary.text)

ØªØ±Ø¬Ù…Ø©

Python

# ØªØ±Ø¬Ù…Ø© Ù†Øµ
translation = client.translate(
    text="Artificial intelligence is transforming education",
    source_language="en",
    target_language="ar"
)

print(f"Original: {translation.source_text}")
print(f"Translated: {translation.translated_text}")
print(f"Confidence: {translation.confidence:.2%}")

# ØªØ±Ø¬Ù…Ø© ÙˆØ«ÙŠÙ‚Ø© ÙƒØ§Ù…Ù„Ø©
translated_doc = client.documents.translate(
    document_id="doc_abc123",
    target_language="ar",
    preserve_formatting=True
)

print(f"Translated document ID: {translated_doc.id}")

6. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª

Python

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø©
collection = client.collections.create(
    name="research-2024",
    description="AI research papers from 2024",
    embedding_model="text-embedding-ada-002",
    metadata={
        "department": "Computer Science",
        "project": "AI in Education"
    }
)

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
collections = client.collections.list()
for coll in collections:
    print(f"- {coll.name}: {coll.document_count} documents")

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø©
collection = client.collections.get("research-2024")
print(f"Name: {collection.name}")
print(f"Documents: {collection.document_count}")
print(f"Size: {collection.total_size_bytes / 1024 / 1024:.2f} MB")

# ØªØ­Ø¯ÙŠØ« Ù…Ø¬Ù…ÙˆØ¹Ø©
collection = client.collections.update(
    name="research-2024",
    description="Updated description"
)

# Ø­Ø°Ù Ù…Ø¬Ù…ÙˆØ¹Ø©
client.collections.delete("old-collection")

7. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡

Python

from bisheng.exceptions import (
    BishengAPIError,
    DocumentNotFoundError,
    AuthenticationError,
    RateLimitError,
    ValidationError
)

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©
try:
    document = client.documents.get("invalid_id")
except DocumentNotFoundError:
    print("Document not found")
except AuthenticationError:
    print("Invalid credentials")
except RateLimitError as e:
    print(f"Rate limit exceeded. Retry after {e.retry_after} seconds")
except BishengAPIError as e:
    print(f"API Error: {e.message}")
    print(f"Status Code: {e.status_code}")
    print(f"Request ID: {e.request_id}")

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø´Ø§Ù…Ù„Ø©
try:
    result = client.search.text("query", "collection")
except Exception as e:
    print(f"Unexpected error: {e}")

Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©

Python

from bisheng import BishengClient
from bisheng.retry import RetryConfig

# ØªÙƒÙˆÙŠÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
retry_config = RetryConfig(
    max_retries=3,
    backoff_factor=2,  # ØªØ£Ø®ÙŠØ± Ù…ØªØµØ§Ø¹Ø¯: 2, 4, 8 Ø«ÙˆØ§Ù†ÙŠ
    retry_on_status=[500, 502, 503, 504]
)

client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    api_key="your-api-key",
    retry_config=retry_config
)

8. Ø£Ù…Ø«Ù„Ø© Ù…ØªÙ‚Ø¯Ù…Ø©
Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚

Python

import asyncio
from pathlib import Path

async def process_documents_batch(file_paths, collection):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
    tasks = []
    
    for file_path in file_paths:
        task = client.documents.upload_async(
            file_path=file_path,
            collection=collection
        )
        tasks.append(task)
    
    # Ø§Ù†ØªØ¸Ø§Ø± ÙƒÙ„ Ø§Ù„Ù…Ù‡Ø§Ù…
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    successful = []
    failed = []
    
    for file_path, result in zip(file_paths, results):
        if isinstance(result, Exception):
            failed.append((file_path, str(result)))
        else:
            successful.append(result)
    
    return successful, failed

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
files = list(Path("./documents").glob("*.pdf"))
successful, failed = asyncio.run(
    process_documents_batch(files, "research-2024")
)

print(f"Successful: {len(successful)}")
print(f"Failed: {len(failed)}")

Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ù…Ø®ØµØµ

Python

class CustomRAG:
    """Ù†Ø¸Ø§Ù… RAG Ù…Ø®ØµØµ"""
    
    def __init__(self, client, collection):
        self.client = client
        self.collection = collection
    
    def ask(self, question, top_k=5, rerank=True):
        """Ø³Ø¤Ø§Ù„ Ù…Ø¹ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø®ØµØµ"""
        # 1. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
        search_results = self.client.search.hybrid(
            query=question,
            collection=self.collection,
            limit=top_k * 2  # Ø¬Ù„Ø¨ Ø£ÙƒØ«Ø± Ù„Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨
        )
        
        # 2. Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if rerank:
            reranked = self._rerank(question, search_results[:top_k])
        else:
            reranked = search_results[:top_k]
        
        # 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
        context = self._build_context(reranked)
        
        # 4. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        answer = self.client.chat.ask(
            question=question,
            collection=self.collection,
            context=context,
            include_sources=True
        )
        
        return answer
    
    def _rerank(self, query, results):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        # ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ reranking Ù…Ø®ØµØµ
        return sorted(results, key=lambda x: x.combined_score, reverse=True)
    
    def _build_context(self, results):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        context_parts = []
        for i, result in enumerate(results, 1):
            context_parts.append(
                f"[Document {i}: {result.filename}, Page {result.page}]\n"
                f"{result.text}\n"
            )
        return "\n".join(context_parts)

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
rag = CustomRAG(client, "research-2024")
answer = rag.ask("What are the benefits of AI in education?")
print(answer.text)

Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡

Python

import time
from contextlib import contextmanager

@contextmanager
def timer(name):
    """Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°"""
    start = time.time()
    yield
    end = time.time()
    print(f"{name}: {end - start:.2f}s")

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
with timer("Document upload"):
    doc = client.documents.upload("large-file.pdf")

with timer("Search"):
    results = client.search.text("query", "collection")

with timer("Question answering"):
    answer = client.chat.ask("question", "collection")

9. Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Frameworks
LangChain Integration

Python

from langchain.document_loaders import BishengLoader
from langchain.vectorstores import BishengVectorStore
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
loader = BishengLoader(
    client=client,
    collection="research-2024"
)
documents = loader.load()

# Ø¥Ù†Ø´Ø§Ø¡ vector store
vectorstore = BishengVectorStore(
    client=client,
    collection="research-2024"
)

# Ø¥Ù†Ø´Ø§Ø¡ QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„
result = qa_chain({"query": "What is AI?"})
print(result["result"])

LlamaIndex Integration

Python

from llama_index import BishengReader, GPTVectorStoreIndex

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
reader = BishengReader(client=client)
documents = reader.load_data(collection="research-2024")

# Ø¥Ù†Ø´Ø§Ø¡ index
index = GPTVectorStoreIndex.from_documents(documents)

# Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
query_engine = index.as_query_engine()
response = query_engine.query("What are AI applications?")
print(response)

10. Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª
Ø§Ø³ØªØ®Ø¯Ø§Ù… Connection Pooling

Python

from bisheng import BishengClient
from bisheng.session import PooledSession

# Ø¥Ù†Ø´Ø§Ø¡ session Ù…Ø¹ connection pooling
session = PooledSession(
    pool_connections=10,
    pool_maxsize=20
)

client = BishengClient(
    api_url="https://your-domain.com/api/v1",
    api_key="your-api-key",
    session=session
)

Caching

Python

from functools import lru_cache

@lru_cache(maxsize=1000)
def search_cached(query, collection):
    """Ø¨Ø­Ø« Ù…Ø¹ caching"""
    results = client.search.text(query, collection)
    return results

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
results1 = search_cached("AI", "research")  # Ù…Ù† API
results2 = search_cached("AI", "research")  # Ù…Ù† cache

Batch Processing

Python

def process_in_batches(items, batch_size=10):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª"""
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        yield batch

# Ø§Ø³ØªØ®Ø¯Ø§Ù…
documents = list(Path("./docs").glob("*.pdf"))
for batch in process_in_batches(documents, batch_size=5):
    for doc in batch:
        client.documents.upload(doc, collection="research")
    time.sleep(1)  # ØªØ¬Ù†Ø¨ rate limiting

ğŸ“– Ù…Ø±Ø§Ø¬Ø¹ Ø¥Ø¶Ø§ÙÙŠØ©

    API Reference
    GitHub Repository
    Examples
    Changelog

